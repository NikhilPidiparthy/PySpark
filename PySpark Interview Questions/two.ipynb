{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Merge DataFrame in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Way\n",
    "By adding extra column with null values withColumn, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"two\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+\n",
      "|             Name|Age|\n",
      "+-----------------+---+\n",
      "|Nikhil,Pidiparthy| 26|\n",
      "|      Akhil,Peddi| 31|\n",
      "|         Sony,Bil| 35|\n",
      "|         Demo,Raj| 21|\n",
      "|      Nibba,Nibbi| 16|\n",
      "+-----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv('DataSets/file1.csv',header=True,sep='|')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mquote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mescape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mignoreLeadingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnullValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnanValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpositiveInf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnegativeInf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdateFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtimestampFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmaxColumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmaxCharsPerColumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmaxMalformedLogPerPartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcolumnNameOfCorruptRecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmultiLine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msamplingRatio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0menforceSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0memptyValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlocale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpathGlobFilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrecursiveFileLookup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodifiedBefore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodifiedAfter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0munescapedQuoteHandling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      "\n",
      "This function will go through the input once to determine the input schema if\n",
      "``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      "``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      "\n",
      ".. versionadded:: 2.0.0\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "path : str or list\n",
      "    string, or list of strings, for input path(s),\n",
      "    or RDD of Strings storing CSV rows.\n",
      "schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      "    an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "    or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "\n",
      "Other Parameters\n",
      "----------------\n",
      "Extra options\n",
      "    For the extra options, refer to\n",
      "    `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "    in the version you use.\n",
      "\n",
      "    .. # noqa\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      ">>> df.dtypes\n",
      "[('_c0', 'string'), ('_c1', 'string')]\n",
      ">>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
      ">>> df2 = spark.read.csv(rdd)\n",
      ">>> df2.dtypes\n",
      "[('_c0', 'string'), ('_c1', 'string')]\n",
      "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "spark.read.csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+\n",
      "|       Name|Age|Gender|\n",
      "+-----------+---+------+\n",
      "|Tamar,Reddy| 26|  Male|\n",
      "| Roti,Pedda| 11|Female|\n",
      "|  Billa,Pow| 35|  Male|\n",
      "|  Kiddy,Bid| 21|  Male|\n",
      "|Vaddi,Naddi| 16|Female|\n",
      "+-----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.csv('DataSets/file2.csv',header=True,sep='|')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Union can only be performed on tables with the same number of columns, but the first table has 2 columns and the second table has 3 columns;\n'Union false, false\n:- Relation [Name#223,Age#224] csv\n+- Relation [Name#254,Age#255,Gender#256] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:1844\u001b[0m, in \u001b[0;36mDataFrame.union\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1834\u001b[0m \u001b[39m@since\u001b[39m(\u001b[39m2.0\u001b[39m)\n\u001b[1;32m   1835\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munion\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m   1836\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Return a new :class:`DataFrame` containing union of rows in this and another\u001b[39;00m\n\u001b[1;32m   1837\u001b[0m \u001b[39m    :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1838\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[39m    Also as standard in SQL, this function resolves columns by position (not by name).\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1844\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49munion(other\u001b[39m.\u001b[39;49m_jdf), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Union can only be performed on tables with the same number of columns, but the first table has 2 columns and the second table has 3 columns;\n'Union false, false\n:- Relation [Name#223,Age#224] csv\n+- Relation [Name#254,Age#255,Gender#256] csv\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+------+\n",
      "|             Name|Age|Gender|\n",
      "+-----------------+---+------+\n",
      "|Nikhil,Pidiparthy| 26|  null|\n",
      "|      Akhil,Peddi| 31|  null|\n",
      "|         Sony,Bil| 35|  null|\n",
      "|         Demo,Raj| 21|  null|\n",
      "|      Nibba,Nibbi| 16|  null|\n",
      "+-----------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_1 = df1.withColumn('Gender',lit(None))\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+------+\n",
      "|             Name|Age|Gender|\n",
      "+-----------------+---+------+\n",
      "|Nikhil,Pidiparthy| 26|  null|\n",
      "|      Akhil,Peddi| 31|  null|\n",
      "|         Sony,Bil| 35|  null|\n",
      "|         Demo,Raj| 21|  null|\n",
      "|      Nibba,Nibbi| 16|  null|\n",
      "|      Tamar,Reddy| 26|  Male|\n",
      "|       Roti,Pedda| 11|Female|\n",
      "|        Billa,Pow| 35|  Male|\n",
      "|        Kiddy,Bid| 21|  Male|\n",
      "|      Vaddi,Naddi| 16|Female|\n",
      "+-----------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.union(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second way\n",
    "Applying own schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+------+\n",
      "|             Name|Age|Gender|\n",
      "+-----------------+---+------+\n",
      "|Nikhil,Pidiparthy| 26|  null|\n",
      "|      Akhil,Peddi| 31|  null|\n",
      "|         Sony,Bil| 35|  null|\n",
      "|         Demo,Raj| 21|  null|\n",
      "|      Nibba,Nibbi| 16|  null|\n",
      "+-----------------+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/23 10:33:40 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 2, schema size: 3\n",
      "CSV file: file:///config/workspace/PySpark%20Interview%20Questions/file1.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "\n",
    "schema = StructType (\n",
    "[\n",
    "StructField('Name',StringType(),True),\n",
    "StructField('Age',IntegerType(),True),\n",
    "StructField('Gender',StringType(),True)\n",
    "]\n",
    ")\n",
    "\n",
    "df3 = spark.read.csv('DataSets/file1.csv',sep ='|',header = True, schema=schema)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+\n",
      "|       Name|Age|Gender|\n",
      "+-----------+---+------+\n",
      "|Tamar,Reddy| 26|  Male|\n",
      "| Roti,Pedda| 11|Female|\n",
      "|  Billa,Pow| 35|  Male|\n",
      "|  Kiddy,Bid| 21|  Male|\n",
      "|Vaddi,Naddi| 16|Female|\n",
      "+-----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = spark.read.csv('DataSets/file2.csv',sep ='|',header = True, schema=schema)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+------+\n",
      "|             Name|Age|Gender|\n",
      "+-----------------+---+------+\n",
      "|Nikhil,Pidiparthy| 26|  null|\n",
      "|      Akhil,Peddi| 31|  null|\n",
      "|         Sony,Bil| 35|  null|\n",
      "|         Demo,Raj| 21|  null|\n",
      "|      Nibba,Nibbi| 16|  null|\n",
      "|      Tamar,Reddy| 26|  Male|\n",
      "|       Roti,Pedda| 11|Female|\n",
      "|        Billa,Pow| 35|  Male|\n",
      "|        Kiddy,Bid| 21|  Male|\n",
      "|      Vaddi,Naddi| 16|Female|\n",
      "+-----------------+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/23 10:33:48 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 2, schema size: 3\n",
      "CSV file: file:///config/workspace/PySpark%20Interview%20Questions/file1.csv\n"
     ]
    }
   ],
   "source": [
    "df3.union(df4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thrid Way\n",
    "\n",
    "with Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = spark.read.csv('DataSets/file1.csv',header=True,sep='|')\n",
    "\n",
    "df6 = spark.read.csv('DataSets/file2.csv',header=True,sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+\n",
      "|             Name|Age|\n",
      "+-----------------+---+\n",
      "|Nikhil,Pidiparthy| 26|\n",
      "|      Akhil,Peddi| 31|\n",
      "|         Sony,Bil| 35|\n",
      "|         Demo,Raj| 21|\n",
      "|      Nibba,Nibbi| 16|\n",
      "+-----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+\n",
      "|       Name|Age|Gender|\n",
      "+-----------+---+------+\n",
      "|Tamar,Reddy| 26|  Male|\n",
      "| Roti,Pedda| 11|Female|\n",
      "|  Billa,Pow| 35|  Male|\n",
      "|  Kiddy,Bid| 21|  Male|\n",
      "|Vaddi,Naddi| 16|Female|\n",
      "+-----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+------+\n",
      "|             Name|Age|Gender|\n",
      "+-----------------+---+------+\n",
      "|      Akhil,Peddi| 31|  null|\n",
      "|        Billa,Pow| 35|  Male|\n",
      "|         Demo,Raj| 21|  null|\n",
      "|        Kiddy,Bid| 21|  Male|\n",
      "|      Nibba,Nibbi| 16|  null|\n",
      "|Nikhil,Pidiparthy| 26|  null|\n",
      "|       Roti,Pedda| 11|Female|\n",
      "|         Sony,Bil| 35|  null|\n",
      "|      Tamar,Reddy| 26|  Male|\n",
      "|      Vaddi,Naddi| 16|Female|\n",
      "+-----------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_o = df5.join(df6,on = ['Name','Age'],how = 'Outer')\n",
    "df_o.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Way\n",
    "For more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', 'Age']\n",
      "['Name', 'Age', 'Gender']\n"
     ]
    }
   ],
   "source": [
    "df7 = spark.read.csv('DataSets/file1.csv',header=True,sep='|')\n",
    "print(df7.columns)\n",
    "\n",
    "df8 = spark.read.csv('DataSets/file2.csv',header=True,sep='|')\n",
    "print(df8.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = set(df7.columns) - set(df8.columns)\n",
    "list_2 = set(df8.columns) - set(df7.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "{'Gender'}\n"
     ]
    }
   ],
   "source": [
    "print(list_1)\n",
    "print(list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+\n",
      "|       Name|Age|Gender|\n",
      "+-----------+---+------+\n",
      "|Tamar,Reddy| 26|  Male|\n",
      "| Roti,Pedda| 11|Female|\n",
      "|  Billa,Pow| 35|  Male|\n",
      "|  Kiddy,Bid| 21|  Male|\n",
      "|Vaddi,Naddi| 16|Female|\n",
      "+-----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each in list_1:\n",
    "    df8 = df8.withColumn(each,lit(None))\n",
    "df8.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+------+\n",
      "|             Name|Age|Gender|\n",
      "+-----------------+---+------+\n",
      "|Nikhil,Pidiparthy| 26|  null|\n",
      "|      Akhil,Peddi| 31|  null|\n",
      "|         Sony,Bil| 35|  null|\n",
      "|         Demo,Raj| 21|  null|\n",
      "|      Nibba,Nibbi| 16|  null|\n",
      "+-----------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each in list_2:\n",
    "    df7 = df7.withColumn(each,lit(None))\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+------+\n",
      "|             Name|Age|Gender|\n",
      "+-----------------+---+------+\n",
      "|Nikhil,Pidiparthy| 26|  null|\n",
      "|      Akhil,Peddi| 31|  null|\n",
      "|         Sony,Bil| 35|  null|\n",
      "|         Demo,Raj| 21|  null|\n",
      "|      Nibba,Nibbi| 16|  null|\n",
      "|      Tamar,Reddy| 26|  Male|\n",
      "|       Roti,Pedda| 11|Female|\n",
      "|        Billa,Pow| 35|  Male|\n",
      "|        Kiddy,Bid| 21|  Male|\n",
      "|      Vaddi,Naddi| 16|Female|\n",
      "+-----------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Fdf = df7.union(df8)\n",
    "Fdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
